import streamlit as st
import os
import json
import glob
import pandas as pd
import time
import zipfile
import io
from main import run_analysis, INPUT_DIR, OUTPUT_DIR, get_chat_metadata
from utils import sanitize_line, parse_log_date

st.set_page_config(page_title="WhatsApp Chat Analytics", page_icon="ðŸ“Š", layout="wide")

@st.cache_data(ttl=60)
def load_chat_files():
    if not os.path.exists(OUTPUT_DIR): return []
    meta_files = glob.glob(os.path.join(OUTPUT_DIR, "*_metadata.json"))
    chat_info = []
    for f in meta_files:
        with open(f, 'r', encoding='utf-8') as m:
            chat_info.append(json.load(m))
    return sorted(chat_info, key=lambda x: x.get('last_date', ''), reverse=True)

@st.cache_data(show_spinner=False)
def load_data(identity):
    json_path = os.path.join(OUTPUT_DIR, f"chat_{identity}_structured.json")
    summary_path = os.path.join(OUTPUT_DIR, f"chat_{identity}_resumo.md")

    data, summary = [], ""
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except: pass

    if os.path.exists(summary_path):
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary = f.read()
        except: pass

    return data, summary

@st.cache_data(show_spinner=False)
def get_processed_df(data):
    if not data: return pd.DataFrame()
    df = pd.DataFrame(data)
    if 'timestamp_iso' in df.columns:
        df['dt'] = pd.to_datetime(df['timestamp_iso'])
    else:
        def parse_dt(ts):
            dt, _ = parse_log_date(ts)
            return pd.to_datetime(dt) if dt else pd.to_datetime(ts, errors='coerce')
        df['dt'] = df['timestamp'].apply(parse_dt)
    df['hour'] = df['dt'].dt.hour
    df['day_name'] = df['dt'].dt.day_name()
    return df

st.sidebar.title("ðŸš€ Processamento")
uploaded_files = st.sidebar.file_uploader(
    "1. Upload de arquivos (ZIP ou .txt + Ã¡udios)",
    accept_multiple_files=True,
    type=["txt", "ogg", "opus", "m4a", "wav", "zip"]
)

if uploaded_files and st.sidebar.button("ðŸ”¥ Iniciar AnÃ¡lise Completa", use_container_width=True):
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    def update_progress(progress, message="Processando..."):
        progress_bar.progress(progress)
        status_text.text(f"{message} {int(progress * 100)}%")

    with st.spinner("Analisando..."):
        if not os.path.exists(INPUT_DIR): os.makedirs(INPUT_DIR)
        files_to_process = []
        for uploaded_file in uploaded_files:
            if uploaded_file.name.endswith(".zip"):
                with zipfile.ZipFile(io.BytesIO(uploaded_file.getvalue())) as z:
                    for zinfo in z.infolist():
                        if zinfo.is_dir() or "__MACOSX" in zinfo.filename or zinfo.filename.startswith("."):
                            continue
                        with z.open(zinfo) as zf:
                            files_to_process.append({"name": os.path.basename(zinfo.filename), "content": zf.read()})
            else:
                files_to_process.append({"name": uploaded_file.name, "content": uploaded_file.getvalue()})

        for item in files_to_process:
            fname, content = item["name"], item["content"]
            full_input_path = os.path.join(INPUT_DIR, fname)

            # Salva o arquivo primeiro para podermos ler metadados sem manter na RAM
            with open(full_input_path, "wb") as f:
                f.write(content)

            if fname.endswith(".txt"):
                meta = get_chat_metadata(full_input_path)
                if meta:
                    identity = meta['identity']
                    new_fname = f"chat_{identity}.txt"
                    new_path = os.path.join(INPUT_DIR, new_fname)

                    # Renomeia se necessÃ¡rio
                    if full_input_path != new_path:
                        if os.path.exists(new_path): os.remove(new_path)
                        os.rename(full_input_path, new_path)

                    meta_path = os.path.join(OUTPUT_DIR, f"chat_{identity}_metadata.json")
                    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
                    with open(meta_path, 'w', encoding='utf-8') as f:
                        json.dump(meta, f, indent=4, ensure_ascii=False)

        if run_analysis(progress_callback=update_progress):
            st.sidebar.success("Pronto!")
            time.sleep(1)
            st.rerun()
        else:
            st.sidebar.error("Erro no processamento.")

st.sidebar.markdown("---")
st.title("ðŸ“Š WhatsApp Chat Analytics")
st.sidebar.title("ðŸ§ Visualizar")
chat_list = load_chat_files()

if not chat_list:
    st.info("Nenhum chat processado.")
else:
    titles_map = {chat['title']: chat['identity'] for chat in chat_list}
    selected_title = st.sidebar.selectbox("Selecionar Conversa", list(titles_map.keys()))

    if st.sidebar.button("ðŸ—‘ï¸ Deletar Conversa Selecionada"):
        if selected_title:
            identity = titles_map[selected_title]
            for f in glob.glob(os.path.join(OUTPUT_DIR, f"chat_{identity}*")):
                try: os.remove(f)
                except: pass
            input_file = os.path.join(INPUT_DIR, f"chat_{identity}.txt")
            if os.path.exists(input_file):
                try: os.remove(input_file)
                except: pass
            st.cache_data.clear()
            st.rerun()

    if selected_title:
        identity = titles_map[selected_title]
        data, summary = load_data(identity)

        if not data:
            st.info("Aguardando processamento... Os arquivos estÃ£o sendo gerados.")
            if st.button("ðŸ”„ Verificar novamente"):
                st.cache_data.clear()
                st.rerun()
            st.stop()

        df = get_processed_df(data)
        tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“ Resumo IA", "ðŸ“Š Insights", "ðŸ’¬ Mensagens", "ðŸ“¦ Dados Brutos"])

        with tab1:
            if summary: st.markdown(summary)
            else: st.info("Resumo nÃ£o disponÃ­vel.")

        with tab2:
            st.subheader("AnÃ¡lise de Engajamento")
            m1, m2, m3 = st.columns(3)
            audio_count = len(df[df["type"] == "audio"])
            m1.metric("Total de Mensagens", len(df))
            m2.metric("Ãudios", audio_count)
            m3.metric("Texto", len(df) - audio_count)

            col_a, col_b = st.columns(2)
            with col_a:
                st.write("**Mensagens por Hora**")
                st.bar_chart(df.groupby('hour').size().reindex(range(24), fill_value=0))
            with col_b:
                st.write("**Mensagens por Dia**")
                days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                st.bar_chart(df.groupby('day_name').size().reindex(days_order, fill_value=0))
            st.write("**Top Participantes**")
            st.bar_chart(df['author'].value_counts())

        with tab3:
            col1, col2, col3 = st.columns([1, 1, 1])
            with col1:
                authors = ["Todos"] + sorted(df["author"].unique().tolist())
                sel_author = st.selectbox("Autor", authors)
            with col2:
                search = st.text_input("Buscar", "")
            with col3:
                page_size = st.number_input("Mensagens por pÃ¡gina", min_value=10, max_value=500, value=50)

            f_df = df.copy()
            if sel_author != "Todos": f_df = f_df[f_df["author"] == sel_author]
            if search: f_df = f_df[f_df["message"].str.contains(search, case=False, na=False)]

            total_msg = len(f_df)
            num_pages = (total_msg // page_size) + (1 if total_msg % page_size > 0 else 0)

            if num_pages > 1:
                page = st.select_slider("PÃ¡gina", options=range(1, num_pages + 1), value=1)
            else:
                page = 1

            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size

            paged_df = f_df.iloc[start_idx:end_idx]

            st.write(f"Exibindo {len(paged_df)} mensagens (PÃ¡gina {page} de {num_pages})")
            for idx, row in paged_df.iterrows():
                with st.chat_message("user" if row["author"] == df["author"].unique()[0] else "assistant"):
                    msg_id = row.get("id", idx)
                    st.write(f"**{row['author']}** - *{row['timestamp']}* (ID: {msg_id})")
                    st.write(row["message"])
                    if row["filename"]:
                        file_p = os.path.join(INPUT_DIR, row["filename"])
                        if os.path.exists(file_p):
                            ext = os.path.splitext(row["filename"])[1].lower()
                            if ext in [".opus", ".ogg", ".m4a", ".wav"]:
                                st.audio(file_p)
                            elif ext in [".jpg", ".jpeg", ".png"]:
                                st.image(file_p, caption=row["filename"], width="stretch")
                            elif ext in [".pdf", ".docx"]:
                                # Key Ãºnica garantida usando hash do filename + timestamp + Ã­ndice
                                unique_id = f"{row['filename']}_{row['timestamp']}_{idx}".replace(" ", "_")
                                btn_key = f"dl_{unique_id}"
                                st.download_button(
                                    f"ðŸ“„ Baixar {row['filename']}",
                                    open(file_p, "rb").read(),
                                    row["filename"],
                                    key=btn_key
                                )
                    st.markdown("---")

        with tab4:
            st.subheader("JSON Estruturado")
            c1, c2 = st.columns(2)
            with c1: st.download_button("ðŸ“¥ Baixar JSON", json.dumps(data, indent=4, ensure_ascii=False), f"chat_{identity}.json", "application/json")
            with c2:
                if summary: st.download_button("ðŸ“¥ Baixar Resumo", summary, f"resumo_{identity}.md", "text/markdown")
            st.json(data)
